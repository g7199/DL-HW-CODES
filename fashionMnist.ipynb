{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.layers import *\n",
    "from collections import OrderedDict\n",
    "from common.gradient import numerical_gradient\n",
    "from common.trainer import Trainer\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()\n",
    "print(X_train.max())\n",
    "print(X_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num = np.unique(y_train, axis=0)\n",
    "num = num.shape[0]\n",
    "y_train = np.eye(num)[y_train]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = np.unique(y_test, axis=0)\n",
    "num = num.shape[0]\n",
    "y_test = np.eye(num)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, X_train, y_train, X_test, y_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2994064543605113\n",
      "=== epoch:1, train acc:0.125, test acc:0.113 ===\n",
      "train loss:2.2969476664107433\n",
      "train loss:2.290298036571933\n",
      "train loss:2.280443650557045\n",
      "train loss:2.266592769850869\n",
      "train loss:2.253694651645621\n",
      "train loss:2.2244360315845295\n",
      "train loss:2.1924547579626497\n",
      "train loss:2.1551794207239614\n",
      "train loss:2.104732418242105\n",
      "train loss:2.0533324049405848\n",
      "train loss:1.997900488539371\n",
      "train loss:1.9396078698513777\n",
      "train loss:1.8483585754960268\n",
      "train loss:1.7710527961405713\n",
      "train loss:1.7187800910171926\n",
      "train loss:1.5833417307437379\n",
      "train loss:1.549970989200826\n",
      "train loss:1.4976883647556365\n",
      "train loss:1.322835993528585\n",
      "train loss:1.2490945199808694\n",
      "train loss:1.276173025861903\n",
      "train loss:1.080280620560267\n",
      "train loss:1.1915264038996956\n",
      "train loss:1.1422544057682387\n",
      "train loss:1.080236211944789\n",
      "train loss:1.1108539518923315\n",
      "train loss:1.158492616157045\n",
      "train loss:0.9948541303966054\n",
      "train loss:0.9869303608008373\n",
      "train loss:1.0658707246966732\n",
      "train loss:0.9397610180226619\n",
      "train loss:0.8871928659245735\n",
      "train loss:0.9569217963864834\n",
      "train loss:0.9851238578873334\n",
      "train loss:1.1050930878834762\n",
      "train loss:0.9507052506611494\n",
      "train loss:1.0075597878112865\n",
      "train loss:0.7530131762084175\n",
      "train loss:1.0107339171072742\n",
      "train loss:0.9485138069686131\n",
      "train loss:1.065567386039654\n",
      "train loss:0.9304860264360635\n",
      "train loss:0.7896356129071868\n",
      "train loss:0.9401146632372834\n",
      "train loss:0.8464093539938335\n",
      "train loss:0.9068778622471837\n",
      "train loss:0.8130829289696976\n",
      "train loss:1.1782046729085154\n",
      "train loss:1.0493614112402654\n",
      "train loss:0.96399087439105\n",
      "train loss:0.8014847035002032\n",
      "train loss:0.9353650074537918\n",
      "train loss:1.0026295124366178\n",
      "train loss:0.8807157709930138\n",
      "train loss:0.720004365030661\n",
      "train loss:0.8209977270717885\n",
      "train loss:0.7918511242266081\n",
      "train loss:0.7663008185200374\n",
      "train loss:0.7474629938570156\n",
      "train loss:0.7086035020607128\n",
      "train loss:0.9081770321011704\n",
      "train loss:0.6659105417710743\n",
      "train loss:0.7442421512436216\n",
      "train loss:0.6324006474841505\n",
      "train loss:0.7720174284978972\n",
      "train loss:0.7874454967864692\n",
      "train loss:0.7144842991256783\n",
      "train loss:0.8048548972691719\n",
      "train loss:0.7356368086382962\n",
      "train loss:0.7606640544537627\n",
      "train loss:0.8900340329646066\n",
      "train loss:0.6496067842027803\n",
      "train loss:0.8459851679891753\n",
      "train loss:0.6008368269841385\n",
      "train loss:0.9161744636667847\n",
      "train loss:0.6231774510034772\n",
      "train loss:0.7451426188107635\n",
      "train loss:0.844831826914027\n",
      "train loss:0.809413302328725\n",
      "train loss:0.7705388278508829\n",
      "train loss:0.7744607668039665\n",
      "train loss:0.7081729095618863\n",
      "train loss:0.7389858918400418\n",
      "train loss:0.8975357150901018\n",
      "train loss:0.514789271164198\n",
      "train loss:0.752409686359169\n",
      "train loss:0.5943956430833071\n",
      "train loss:0.7940452765134703\n",
      "train loss:0.7963227855448015\n",
      "train loss:0.670517552350043\n",
      "train loss:0.6423377064924083\n",
      "train loss:0.635442114624389\n",
      "train loss:0.725009362957711\n",
      "train loss:0.6056549271923081\n",
      "train loss:0.6736537077586953\n",
      "train loss:0.8656694691066678\n",
      "train loss:0.5672392077382011\n",
      "train loss:0.8424351165955585\n",
      "train loss:0.5470571768746326\n",
      "train loss:0.6121473596939375\n",
      "train loss:0.6898054478466972\n",
      "train loss:0.6740847168310014\n",
      "train loss:0.6974099185543012\n",
      "train loss:0.5807934737608804\n",
      "train loss:0.7151847121215014\n",
      "train loss:0.8182294668061418\n",
      "train loss:0.6285268135057012\n",
      "train loss:0.7170960842025785\n",
      "train loss:0.7070062901366841\n",
      "train loss:0.7584872963154183\n",
      "train loss:0.5933227517743901\n",
      "train loss:0.6620088117748985\n",
      "train loss:0.6820258054019886\n",
      "train loss:0.6673973294005764\n",
      "train loss:0.6397504922723176\n",
      "train loss:0.6018935679442601\n",
      "train loss:0.6900865279879547\n",
      "train loss:0.5159326764335759\n",
      "train loss:0.7205073179810237\n",
      "train loss:0.5481428022140273\n",
      "train loss:0.6848754778259684\n",
      "train loss:0.6471346769772317\n",
      "train loss:0.6111996467491224\n",
      "train loss:0.7006147404910892\n",
      "train loss:0.6155375141811524\n",
      "train loss:0.40211666397727563\n",
      "train loss:0.6511560590131711\n",
      "train loss:0.6064172645374127\n",
      "train loss:0.6817864949926628\n",
      "train loss:0.5124242458951163\n",
      "train loss:0.7219458278687573\n",
      "train loss:0.7041935454444902\n",
      "train loss:0.6864997861069245\n",
      "train loss:0.6094456703827991\n",
      "train loss:0.712865561142067\n",
      "train loss:0.6905351211189179\n",
      "train loss:0.7533360483986925\n",
      "train loss:0.7468106116486726\n",
      "train loss:0.5432560962737558\n",
      "train loss:0.6726372520486622\n",
      "train loss:0.6296900595127114\n",
      "train loss:0.6941857365149563\n",
      "train loss:0.6965767854674398\n",
      "train loss:0.587738860783903\n",
      "train loss:0.7425749862397997\n",
      "train loss:0.8326021256716019\n",
      "train loss:0.6740216944548542\n",
      "train loss:0.6589221515632837\n",
      "train loss:0.6019756825444282\n",
      "train loss:0.620203233706835\n",
      "train loss:0.7442764051205865\n",
      "train loss:0.6023278259756651\n",
      "train loss:0.5462529244037019\n",
      "train loss:0.7840204441342283\n",
      "train loss:0.5371174388256232\n",
      "train loss:0.5634653118289659\n",
      "train loss:0.7721747090717943\n",
      "train loss:0.58203511042963\n",
      "train loss:0.6975912913630204\n",
      "train loss:0.7536882723565296\n",
      "train loss:0.6730302608851738\n",
      "train loss:0.6666852135482846\n",
      "train loss:0.661626666461445\n",
      "train loss:0.6297458147054497\n",
      "train loss:0.5281696800269812\n",
      "train loss:0.5102734754717376\n",
      "train loss:0.5963371498036649\n",
      "train loss:0.6667110962083421\n",
      "train loss:0.487399198300397\n",
      "train loss:0.5128850933564718\n",
      "train loss:0.6121161868481717\n",
      "train loss:0.6052026094284395\n",
      "train loss:0.605804889989793\n",
      "train loss:0.5300484080479964\n",
      "train loss:0.6354981869968794\n",
      "train loss:0.6169468818788058\n",
      "train loss:0.6746531871988514\n",
      "train loss:0.7248023420872487\n",
      "train loss:0.6243741035085353\n",
      "train loss:0.6540940218968011\n",
      "train loss:0.7323014601637692\n",
      "train loss:0.5105948698982844\n",
      "train loss:0.7223450011921209\n",
      "train loss:0.5651764962928626\n",
      "train loss:0.5784374257984659\n",
      "train loss:0.5911687243748855\n",
      "train loss:0.6304992891445108\n",
      "train loss:0.7312469848898108\n",
      "train loss:0.5880644869877952\n",
      "train loss:0.7643875975056708\n",
      "train loss:0.5137159856747641\n",
      "train loss:0.6965263486318796\n",
      "train loss:0.6203964505929708\n",
      "train loss:0.46706195319692034\n",
      "train loss:0.5845314047588114\n",
      "train loss:0.6910012218800392\n",
      "train loss:0.5544390485922197\n",
      "train loss:0.5538912172751835\n",
      "train loss:0.4903825728298949\n",
      "train loss:0.5335812164943299\n",
      "train loss:0.5259459521531032\n",
      "train loss:0.5698754343651724\n",
      "train loss:0.5649562129661638\n",
      "train loss:0.6380145168863167\n",
      "train loss:0.5622034733532704\n",
      "train loss:0.5404717525703968\n",
      "train loss:0.47857130434981415\n",
      "train loss:0.5167623065817945\n",
      "train loss:0.5597758237789728\n",
      "train loss:0.6083616869652031\n",
      "train loss:0.5372667403869709\n",
      "train loss:0.4099923015306695\n",
      "train loss:0.7602409314919992\n",
      "train loss:0.5111659141787332\n",
      "train loss:0.5443230939788727\n",
      "train loss:0.5779080714430581\n",
      "train loss:0.4801507502969056\n",
      "train loss:0.4526985193358089\n",
      "train loss:0.5488007141761004\n",
      "train loss:0.6651780722449441\n",
      "train loss:0.5120023890996852\n",
      "train loss:0.6303671273597479\n",
      "train loss:0.5926379187813007\n",
      "train loss:0.5103986389752995\n",
      "train loss:0.5622076666048285\n",
      "train loss:0.5873150836142542\n",
      "train loss:0.44591942093881054\n",
      "train loss:0.48745220045376425\n",
      "train loss:0.4910156607688147\n",
      "train loss:0.5328297567633095\n",
      "train loss:0.41054118038591036\n",
      "train loss:0.5390908917689983\n",
      "train loss:0.5457827435472036\n",
      "train loss:0.47346842596757427\n",
      "train loss:0.5544917072715575\n",
      "train loss:0.5788014163137438\n",
      "train loss:0.5976452971634039\n",
      "train loss:0.7278681664828902\n",
      "train loss:0.504855381263138\n",
      "train loss:0.5263058698954795\n",
      "train loss:0.5352704523461942\n",
      "train loss:0.48169910935710997\n",
      "train loss:0.5632162836770775\n",
      "train loss:0.49220526875298987\n",
      "train loss:0.43994151683617533\n",
      "train loss:0.4427863856619366\n",
      "train loss:0.5106716995359097\n",
      "train loss:0.5153817245183842\n",
      "train loss:0.4940328862061493\n",
      "train loss:0.46130204048388923\n",
      "train loss:0.7669846939802646\n",
      "train loss:0.5569888079285813\n",
      "train loss:0.42990138881021517\n",
      "train loss:0.5145147195399854\n",
      "train loss:0.41755867057770585\n",
      "train loss:0.8418799427722601\n",
      "train loss:0.5685311607816083\n",
      "train loss:0.5230502169476378\n",
      "train loss:0.5379204724328883\n",
      "train loss:0.6059470027957402\n",
      "train loss:0.4929681665646582\n",
      "train loss:0.5412893382135341\n",
      "train loss:0.6332037434776138\n",
      "train loss:0.5062165486160383\n",
      "train loss:0.6504998648213991\n",
      "train loss:0.5211220436913827\n",
      "train loss:0.44031981378142876\n",
      "train loss:0.5193755587220485\n",
      "train loss:0.4774806799917558\n",
      "train loss:0.7066304974261242\n",
      "train loss:0.5621625335229112\n",
      "train loss:0.6900017776562893\n",
      "train loss:0.6056838442291618\n",
      "train loss:0.4014371332230612\n",
      "train loss:0.5447335166575623\n",
      "train loss:0.42689817356934207\n",
      "train loss:0.591696647478627\n",
      "train loss:0.5811179334077817\n",
      "train loss:0.5445364081051889\n",
      "train loss:0.40553485267739303\n",
      "train loss:0.5171755447098932\n",
      "train loss:0.629008421265981\n",
      "train loss:0.4427697218927703\n",
      "train loss:0.5231826647567076\n",
      "train loss:0.6051250890650087\n",
      "train loss:0.6087029834712057\n",
      "train loss:0.5617621877453094\n",
      "train loss:0.5871262911520226\n",
      "train loss:0.49892759045610124\n",
      "train loss:0.47375929460673\n",
      "train loss:0.5429336028667757\n",
      "train loss:0.629987133143402\n",
      "train loss:0.3935039922566559\n",
      "train loss:0.5493037618767499\n",
      "train loss:0.6292346111678354\n",
      "train loss:0.3679498095545808\n",
      "train loss:0.6288818692455189\n",
      "train loss:0.5574668934895978\n",
      "train loss:0.6374501208001092\n",
      "train loss:0.6337443278808257\n",
      "train loss:0.5177002973008543\n",
      "train loss:0.6204752382898763\n",
      "train loss:0.44587008053401\n",
      "train loss:0.5679173680889197\n",
      "train loss:0.5301362016777218\n",
      "train loss:0.572639540585941\n",
      "train loss:0.5602207333878916\n",
      "train loss:0.5821415798111258\n",
      "train loss:0.6011350183019455\n",
      "train loss:0.6285730408343464\n",
      "train loss:0.5226451634307279\n",
      "train loss:0.5321975702924671\n",
      "train loss:0.4721258391076105\n",
      "train loss:0.696327604596908\n",
      "train loss:0.4500323455661254\n",
      "train loss:0.5602530312258939\n",
      "train loss:0.5648402062354899\n",
      "train loss:0.454252438770225\n",
      "train loss:0.3785165847710625\n",
      "train loss:0.41240616847035516\n",
      "train loss:0.5233325398495289\n",
      "train loss:0.5683838446901815\n",
      "train loss:0.6382780438407611\n",
      "train loss:0.5656582469013519\n",
      "train loss:0.5176137213578113\n",
      "train loss:0.5578313616826504\n",
      "train loss:0.48031525023887467\n",
      "train loss:0.49780879293944624\n",
      "train loss:0.5047422904389266\n",
      "train loss:0.6503886178041476\n",
      "train loss:0.5009315185350655\n",
      "train loss:0.5006447050438232\n",
      "train loss:0.6636218640927994\n",
      "train loss:0.5090301410276724\n",
      "train loss:0.5697439220815881\n",
      "train loss:0.5146480334557773\n",
      "train loss:0.5007641928518729\n",
      "train loss:0.49282748280308325\n",
      "train loss:0.5191543373173326\n",
      "train loss:0.45354534138003466\n",
      "train loss:0.5733000619958325\n",
      "train loss:0.4732469286071639\n",
      "train loss:0.6107456980851044\n",
      "train loss:0.4920494122140668\n",
      "train loss:0.5107859755242935\n",
      "train loss:0.48698749133736363\n",
      "train loss:0.42262025837884015\n",
      "train loss:0.5224154957624567\n",
      "train loss:0.596678440386608\n",
      "train loss:0.485194316607202\n",
      "train loss:0.41369567051032663\n",
      "train loss:0.39648798916182587\n",
      "train loss:0.4273152392598664\n",
      "train loss:0.4155203443893305\n",
      "train loss:0.5184220287155386\n",
      "train loss:0.4814868141790887\n",
      "train loss:0.5079031768122187\n",
      "train loss:0.49509291112774484\n",
      "train loss:0.5194011697687189\n",
      "train loss:0.35051381670415244\n",
      "train loss:0.6256786793172503\n",
      "train loss:0.5897670478856897\n",
      "train loss:0.4575209108872904\n",
      "train loss:0.5119533721210775\n",
      "train loss:0.423929861914395\n",
      "train loss:0.3984692223844273\n",
      "train loss:0.3755432960549676\n",
      "train loss:0.49103367499430967\n",
      "train loss:0.4244308358582937\n",
      "train loss:0.6406193897729715\n",
      "train loss:0.6623522469048815\n",
      "train loss:0.4994852020991046\n",
      "train loss:0.4118635608545675\n",
      "train loss:0.3618415290945661\n",
      "train loss:0.42270479754792867\n",
      "train loss:0.5544307088021044\n",
      "train loss:0.5036712332635485\n",
      "train loss:0.3417573354973903\n",
      "train loss:0.3345154511902353\n",
      "train loss:0.3927388040132835\n",
      "train loss:0.3579046230631105\n",
      "train loss:0.4993755300996133\n",
      "train loss:0.4280832154634545\n",
      "train loss:0.4744979658769839\n",
      "train loss:0.516945024440403\n",
      "train loss:0.5065067044081905\n",
      "train loss:0.519665852436297\n",
      "train loss:0.557926207225379\n",
      "train loss:0.4399769550080441\n",
      "train loss:0.48673464668873606\n",
      "train loss:0.6817106602895582\n",
      "train loss:0.5280211791289875\n",
      "train loss:0.4907737902460908\n",
      "train loss:0.5027331706059083\n",
      "train loss:0.4899559053378553\n",
      "train loss:0.45123282496550327\n",
      "train loss:0.5234653106727862\n",
      "train loss:0.40916233126944335\n",
      "train loss:0.5535022486775528\n",
      "train loss:0.4801637845198801\n",
      "train loss:0.4876352184480013\n",
      "train loss:0.46562545243366393\n",
      "train loss:0.4625500116822067\n",
      "train loss:0.5696947290303003\n",
      "train loss:0.6943851978626933\n",
      "train loss:0.47589564417372765\n",
      "train loss:0.46598042117365956\n",
      "train loss:0.35004521611246764\n",
      "train loss:0.46384177627730183\n",
      "train loss:0.41064203400275956\n",
      "train loss:0.4789470272667718\n",
      "train loss:0.5079992373370267\n",
      "train loss:0.4629457531034479\n",
      "train loss:0.3239517303301131\n",
      "train loss:0.4952056781607297\n",
      "train loss:0.4190123195860469\n",
      "train loss:0.38629752986269444\n",
      "train loss:0.5225470884995282\n",
      "train loss:0.4579609331645252\n",
      "train loss:0.39356824396818735\n",
      "train loss:0.4803365542867734\n",
      "train loss:0.5479847113158902\n",
      "train loss:0.5757646277818798\n",
      "train loss:0.3992045627300822\n",
      "train loss:0.2923909479623052\n",
      "train loss:0.5043995844185383\n",
      "train loss:0.669073560208857\n",
      "train loss:0.3590605247736686\n",
      "train loss:0.3864052987885384\n",
      "train loss:0.5346320874638975\n",
      "train loss:0.5452033065684728\n",
      "train loss:0.40160305244467287\n",
      "train loss:0.6105515872748801\n",
      "train loss:0.4247105793029109\n",
      "train loss:0.37678641370439114\n",
      "train loss:0.4680655463934596\n",
      "train loss:0.4381616372238266\n",
      "train loss:0.49983341810466037\n",
      "train loss:0.5116666679755779\n",
      "train loss:0.5153254871685374\n",
      "train loss:0.4704605820051109\n",
      "train loss:0.4297874646778863\n",
      "train loss:0.37933626546463883\n",
      "train loss:0.6126393907202402\n",
      "train loss:0.40532434576305315\n",
      "train loss:0.544195193157402\n",
      "train loss:0.5137304883614294\n",
      "train loss:0.4982899203540206\n",
      "train loss:0.407104268201033\n",
      "train loss:0.5284250904310924\n",
      "train loss:0.4179298089852852\n",
      "train loss:0.4392566001431254\n",
      "train loss:0.4948775403270491\n",
      "train loss:0.4791246527577559\n",
      "train loss:0.42121794397631795\n",
      "train loss:0.6173942455303277\n",
      "train loss:0.522881150188692\n",
      "train loss:0.4630902362231592\n",
      "train loss:0.48995653033197073\n",
      "train loss:0.4934597688943158\n",
      "train loss:0.5581387890047348\n",
      "train loss:0.5069466388935202\n",
      "train loss:0.4520789315228916\n",
      "train loss:0.6085105969869438\n",
      "train loss:0.40758462964301695\n",
      "train loss:0.5203299807564689\n",
      "train loss:0.34277147203031816\n",
      "train loss:0.4273003398268097\n",
      "train loss:0.4937113641332955\n",
      "train loss:0.38702310595920736\n",
      "train loss:0.37856743484368993\n",
      "train loss:0.5201206827194373\n",
      "train loss:0.4712086559719724\n",
      "train loss:0.5162370110496098\n",
      "train loss:0.45748814608600696\n",
      "train loss:0.5830148538484842\n",
      "train loss:0.4499259414653337\n",
      "train loss:0.516708592587386\n",
      "train loss:0.6640894884090148\n",
      "train loss:0.5153899389658447\n",
      "train loss:0.5046326690560785\n",
      "train loss:0.5098401600377874\n",
      "train loss:0.4429271899137921\n",
      "train loss:0.34362440963279467\n",
      "train loss:0.5569308040672705\n",
      "train loss:0.5472877528897904\n",
      "train loss:0.43950771255942817\n",
      "train loss:0.5215553889599371\n",
      "train loss:0.4017146501992365\n",
      "train loss:0.3843764648908748\n",
      "train loss:0.3769336705502207\n",
      "train loss:0.47402317789628307\n",
      "train loss:0.5728280321380527\n",
      "train loss:0.461573731646865\n",
      "train loss:0.4257613177590794\n",
      "train loss:0.5525480288908979\n",
      "train loss:0.5462209543112285\n",
      "train loss:0.45722977230991907\n",
      "train loss:0.471055087736044\n",
      "train loss:0.605510544290484\n",
      "train loss:0.3433626738208555\n",
      "train loss:0.3626943046762728\n",
      "train loss:0.34883623039027684\n",
      "train loss:0.36663779120821943\n",
      "train loss:0.36162786579725614\n",
      "train loss:0.4493152389222598\n",
      "train loss:0.4054276521536452\n",
      "train loss:0.3545554820556462\n",
      "train loss:0.40238399705272376\n",
      "train loss:0.4206391175996991\n",
      "train loss:0.4170802249147148\n",
      "train loss:0.444329241748024\n",
      "train loss:0.4015890750797713\n",
      "train loss:0.45831346770479614\n",
      "train loss:0.4834225989111842\n",
      "train loss:0.347526003715585\n",
      "train loss:0.38156655189435895\n",
      "train loss:0.34634185548053087\n",
      "train loss:0.6090707921963439\n",
      "train loss:0.3674027546400294\n",
      "train loss:0.47115075010687574\n",
      "train loss:0.5208746391716044\n",
      "train loss:0.366514034417292\n",
      "train loss:0.4488449238557318\n",
      "train loss:0.3648520660404932\n",
      "train loss:0.47120356083985804\n",
      "train loss:0.4638532537778722\n",
      "train loss:0.45569232548288907\n",
      "train loss:0.6566572701874673\n",
      "train loss:0.41381185042073587\n",
      "train loss:0.4287400885822031\n",
      "train loss:0.38438789341328944\n",
      "train loss:0.3938688559082917\n",
      "train loss:0.3699123958287986\n",
      "train loss:0.462324254655735\n",
      "train loss:0.3237386681083012\n",
      "train loss:0.4729020362196221\n",
      "train loss:0.3945874011493303\n",
      "train loss:0.36399509326009827\n",
      "train loss:0.3834945862106216\n",
      "train loss:0.5290362102474405\n",
      "train loss:0.4296070086683864\n",
      "train loss:0.5807217553720757\n",
      "train loss:0.40743340804245937\n",
      "train loss:0.2950435888698352\n",
      "train loss:0.34689453481349125\n",
      "train loss:0.4102756781522555\n",
      "train loss:0.47722141493424075\n",
      "train loss:0.3818434934608165\n",
      "train loss:0.6545272170869874\n",
      "train loss:0.5258113596152658\n",
      "train loss:0.41359837922979387\n",
      "train loss:0.3908258533596572\n",
      "train loss:0.5746202112715427\n",
      "train loss:0.40057803985948653\n",
      "train loss:0.3911880543270254\n",
      "train loss:0.4007223082186476\n",
      "train loss:0.37749130428069066\n",
      "train loss:0.47685758989447\n",
      "train loss:0.3301538799950181\n",
      "train loss:0.3694620981359\n",
      "train loss:0.3996566014284058\n",
      "train loss:0.5314551524712737\n",
      "train loss:0.3633034461649439\n",
      "train loss:0.29904849868474115\n",
      "train loss:0.39707076404176894\n",
      "train loss:0.40289347051816304\n",
      "train loss:0.45511861445995067\n",
      "train loss:0.47389374660362565\n",
      "train loss:0.45878795371713593\n",
      "train loss:0.34018800721856246\n",
      "train loss:0.4866516758710792\n",
      "train loss:0.5022013868423719\n",
      "train loss:0.33526724173465555\n",
      "train loss:0.34992292916022855\n",
      "train loss:0.4897725840634113\n",
      "train loss:0.3103648461572586\n",
      "train loss:0.35866330131516366\n",
      "train loss:0.361761028147557\n",
      "train loss:0.3830331683563951\n",
      "train loss:0.33712640788112724\n",
      "train loss:0.2882292136793495\n",
      "train loss:0.5218526501050893\n",
      "train loss:0.5793329277224013\n",
      "train loss:0.44402958953790345\n",
      "train loss:0.5621836582684988\n",
      "train loss:0.3264824717436127\n",
      "train loss:0.5305018528823828\n",
      "train loss:0.47611296850442153\n",
      "train loss:0.5159490838273083\n",
      "train loss:0.39942105907788106\n",
      "train loss:0.40268180096243633\n",
      "train loss:0.5106041662744114\n",
      "train loss:0.44771590829303193\n",
      "train loss:0.3227850680179953\n",
      "train loss:0.5426491848234869\n",
      "train loss:0.3584411171523254\n",
      "train loss:0.3255047597714531\n",
      "train loss:0.2984943115466973\n",
      "train loss:0.34964578151267434\n",
      "=== epoch:2, train acc:0.861, test acc:0.843 ===\n",
      "train loss:0.35927408241120673\n",
      "train loss:0.43890150070909456\n",
      "train loss:0.5241409568181534\n",
      "train loss:0.5418022908555877\n",
      "train loss:0.4186696375992705\n",
      "train loss:0.37414816921093136\n",
      "train loss:0.3913622687178382\n",
      "train loss:0.3420449384422373\n",
      "train loss:0.4948000563887503\n",
      "train loss:0.342796731754509\n",
      "train loss:0.3937368837455187\n",
      "train loss:0.4822850965212753\n",
      "train loss:0.47066686614246206\n",
      "train loss:0.3685273279090435\n",
      "train loss:0.5217015744711899\n",
      "train loss:0.31700700438534496\n",
      "train loss:0.3871180851935727\n",
      "train loss:0.3117320363959316\n",
      "train loss:0.3537386432143104\n",
      "train loss:0.43033150920354907\n",
      "train loss:0.49367939332051525\n",
      "train loss:0.3872027083087703\n",
      "train loss:0.361182811798021\n",
      "train loss:0.309892662581737\n",
      "train loss:0.3565048473980667\n",
      "train loss:0.45400921003609596\n",
      "train loss:0.39822334853753233\n",
      "train loss:0.3220444805347861\n",
      "train loss:0.46700276018090714\n",
      "train loss:0.5729005077207616\n",
      "train loss:0.46965912173672725\n",
      "train loss:0.3406246577175308\n",
      "train loss:0.4474237744691041\n",
      "train loss:0.44646776882844863\n",
      "train loss:0.403013856423599\n",
      "train loss:0.37351875953260616\n",
      "train loss:0.5284854090568815\n",
      "train loss:0.4062621610087323\n",
      "train loss:0.3778059490842112\n",
      "train loss:0.5109357494084142\n",
      "train loss:0.5958210377291384\n",
      "train loss:0.4275528273153067\n",
      "train loss:0.3121551748695577\n",
      "train loss:0.5302181167617737\n",
      "train loss:0.4257840547675069\n",
      "train loss:0.30451014564664264\n",
      "train loss:0.382666425204709\n",
      "train loss:0.505289726491527\n",
      "train loss:0.38122279497043055\n",
      "train loss:0.4244175699392314\n",
      "train loss:0.3224738767758849\n",
      "train loss:0.4949928602332343\n",
      "train loss:0.42064660141152355\n",
      "train loss:0.4168818249881434\n",
      "train loss:0.44974299213670865\n",
      "train loss:0.4728852260733566\n",
      "train loss:0.3958409371841426\n",
      "train loss:0.41903887840008563\n",
      "train loss:0.33331495415804663\n",
      "train loss:0.4379817666978028\n",
      "train loss:0.3272487365425548\n",
      "train loss:0.2691128451866902\n",
      "train loss:0.4362752884998229\n",
      "train loss:0.4040777706803449\n",
      "train loss:0.5527172587947715\n",
      "train loss:0.4603850637987695\n",
      "train loss:0.30750269539794634\n",
      "train loss:0.5602687364462259\n",
      "train loss:0.34380519758792466\n",
      "train loss:0.38695101325191017\n",
      "train loss:0.492534780262564\n",
      "train loss:0.3185179263084661\n",
      "train loss:0.34766541052592537\n",
      "train loss:0.4083996775619035\n",
      "train loss:0.40426939807721385\n",
      "train loss:0.4004368284971253\n",
      "train loss:0.47591592216767103\n",
      "train loss:0.529670735905242\n",
      "train loss:0.43096239389243474\n",
      "train loss:0.40186780468554645\n",
      "train loss:0.524684444064188\n",
      "train loss:0.2902778428509276\n",
      "train loss:0.24234303136975308\n",
      "train loss:0.4160944202648992\n",
      "train loss:0.3033298736450766\n",
      "train loss:0.5327339010786563\n",
      "train loss:0.2921998324634077\n",
      "train loss:0.6107985695644367\n",
      "train loss:0.3923356064603197\n",
      "train loss:0.3998102348950734\n",
      "train loss:0.3114361047166223\n",
      "train loss:0.30003254792929224\n",
      "train loss:0.4104332039464023\n",
      "train loss:0.42283055459031105\n",
      "train loss:0.3083601580425725\n",
      "train loss:0.3446154632297614\n",
      "train loss:0.5297674121018509\n",
      "train loss:0.4217924917677257\n",
      "train loss:0.40007301471013773\n",
      "train loss:0.3713003484767301\n",
      "train loss:0.5929387525216041\n",
      "train loss:0.3015230794494764\n",
      "train loss:0.24969750573834634\n",
      "train loss:0.5094868323908978\n",
      "train loss:0.32436745921406684\n",
      "train loss:0.5038806135042266\n",
      "train loss:0.4054856601375399\n",
      "train loss:0.40897871947133796\n",
      "train loss:0.4370137613465128\n",
      "train loss:0.2819595934838334\n",
      "train loss:0.3283324955740634\n",
      "train loss:0.2656064661001263\n",
      "train loss:0.37813291005646027\n",
      "train loss:0.27742454169163905\n",
      "train loss:0.3554756538275899\n",
      "train loss:0.37011480793088547\n",
      "train loss:0.5314719653860469\n",
      "train loss:0.3300225487172582\n",
      "train loss:0.3622386445946485\n",
      "train loss:0.3278240313103143\n",
      "train loss:0.42661474667483895\n",
      "train loss:0.3865434447118536\n",
      "train loss:0.3515635317366989\n",
      "train loss:0.32610199492693587\n",
      "train loss:0.3779171372195467\n",
      "train loss:0.5849283662030436\n",
      "train loss:0.31600332932623504\n",
      "train loss:0.3844384413862748\n",
      "train loss:0.6080581009368367\n",
      "train loss:0.3279008576046077\n",
      "train loss:0.36141818068679343\n",
      "train loss:0.35279599581183446\n",
      "train loss:0.27687235716098085\n",
      "train loss:0.5237722763043687\n",
      "train loss:0.3414591046460942\n",
      "train loss:0.42687015300705766\n",
      "train loss:0.40332070275323845\n",
      "train loss:0.34302945875922775\n",
      "train loss:0.27176592719448256\n",
      "train loss:0.3414325582711162\n",
      "train loss:0.29555677240216766\n",
      "train loss:0.40768837059384433\n",
      "train loss:0.400723557069774\n",
      "train loss:0.4032352381803992\n",
      "train loss:0.4346475993111699\n",
      "train loss:0.3611888934408282\n",
      "train loss:0.40233107964191817\n",
      "train loss:0.4351765738300211\n",
      "train loss:0.39887861495053967\n",
      "train loss:0.2958602860552035\n",
      "train loss:0.2869034547329328\n",
      "train loss:0.4473094116183116\n",
      "train loss:0.27056068027438684\n",
      "train loss:0.3676667086723273\n",
      "train loss:0.35678358754512324\n",
      "train loss:0.42438174117316024\n",
      "train loss:0.3134798225178177\n",
      "train loss:0.4285627532502893\n",
      "train loss:0.45694160676347323\n",
      "train loss:0.33089005528920745\n",
      "train loss:0.3400792723896743\n",
      "train loss:0.4656172560834718\n",
      "train loss:0.412063958374918\n",
      "train loss:0.422858159650981\n",
      "train loss:0.42469018634349837\n",
      "train loss:0.3717142891281619\n",
      "train loss:0.33903186246895667\n",
      "train loss:0.3979168700199472\n",
      "train loss:0.3667319456471565\n",
      "train loss:0.5043693292452878\n",
      "train loss:0.32224643488737414\n",
      "train loss:0.2695185556573873\n",
      "train loss:0.310440596360467\n",
      "train loss:0.37956619691495375\n",
      "train loss:0.40558897342129235\n",
      "train loss:0.4086380712990849\n",
      "train loss:0.5770113789787432\n",
      "train loss:0.4308697325624829\n",
      "train loss:0.43507196352439237\n",
      "train loss:0.35979942876190313\n",
      "train loss:0.3009166258020569\n",
      "train loss:0.44199407347081165\n",
      "train loss:0.33271641130096014\n",
      "train loss:0.4379691121526087\n",
      "train loss:0.33892490729803204\n",
      "train loss:0.4184281175734018\n",
      "train loss:0.5721496511034365\n",
      "train loss:0.42477915910799796\n",
      "train loss:0.2914607600685199\n",
      "train loss:0.32217179541839186\n",
      "train loss:0.43982839401434587\n",
      "train loss:0.4046963589682798\n",
      "train loss:0.42694123374413573\n",
      "train loss:0.3167552067786932\n",
      "train loss:0.45010267612698796\n",
      "train loss:0.26395977927442754\n",
      "train loss:0.5828142998468533\n",
      "train loss:0.3559624425956003\n",
      "train loss:0.42386302001467446\n",
      "train loss:0.42042365924538677\n",
      "train loss:0.3788027438413772\n",
      "train loss:0.3132651420320738\n",
      "train loss:0.3220793077304859\n",
      "train loss:0.4835583991139093\n",
      "train loss:0.2718387742448645\n",
      "train loss:0.3718499513081632\n",
      "train loss:0.3483383552474759\n",
      "train loss:0.36110884898854434\n",
      "train loss:0.4403013761907388\n",
      "train loss:0.307401707730252\n",
      "train loss:0.34043468198122395\n",
      "train loss:0.44734680485741796\n",
      "train loss:0.29847290793920456\n",
      "train loss:0.24787612184136104\n",
      "train loss:0.35323913546389835\n",
      "train loss:0.3309065275523578\n",
      "train loss:0.42091193114704695\n",
      "train loss:0.36436123999933434\n",
      "train loss:0.44926767643807203\n",
      "train loss:0.35742762573187264\n",
      "train loss:0.32257630199843973\n",
      "train loss:0.300260497894505\n",
      "train loss:0.4323366221167049\n",
      "train loss:0.28793772571215936\n",
      "train loss:0.34747844251559984\n",
      "train loss:0.2845151838148973\n",
      "train loss:0.3588651273371031\n",
      "train loss:0.37929509152913377\n",
      "train loss:0.3332536085895469\n",
      "train loss:0.5885327567118186\n",
      "train loss:0.3593200174452278\n",
      "train loss:0.3613632125892536\n",
      "train loss:0.4004023704598591\n",
      "train loss:0.3642328974986669\n",
      "train loss:0.4412066301574852\n",
      "train loss:0.2556514042215087\n",
      "train loss:0.4164347354738651\n",
      "train loss:0.3988757762157682\n",
      "train loss:0.515477822714398\n",
      "train loss:0.37735678169347514\n",
      "train loss:0.29403411175002825\n",
      "train loss:0.20634788795751885\n",
      "train loss:0.4189112573309683\n",
      "train loss:0.31967443869999673\n",
      "train loss:0.27856708593711177\n",
      "train loss:0.4628815551841441\n",
      "train loss:0.4258742691303724\n",
      "train loss:0.404071488208456\n",
      "train loss:0.3793319358983061\n",
      "train loss:0.411591884755618\n",
      "train loss:0.6875268408668265\n",
      "train loss:0.26652928189039243\n",
      "train loss:0.4463655964178197\n",
      "train loss:0.5547880156706354\n",
      "train loss:0.3562490849758777\n",
      "train loss:0.3350927426310185\n",
      "train loss:0.5350866157325891\n",
      "train loss:0.3759148019486147\n",
      "train loss:0.3619826432106192\n",
      "train loss:0.34716956882515937\n",
      "train loss:0.4029625766254743\n",
      "train loss:0.3057070345520771\n",
      "train loss:0.37155392984543517\n",
      "train loss:0.6214215633036533\n",
      "train loss:0.2824346205289395\n",
      "train loss:0.362086870080262\n",
      "train loss:0.4117889643893441\n",
      "train loss:0.3524415707229499\n",
      "train loss:0.2809689594971966\n",
      "train loss:0.3295972450863153\n",
      "train loss:0.42790231793951894\n",
      "train loss:0.256002452319134\n",
      "train loss:0.3745749395233012\n",
      "train loss:0.4990879038337607\n",
      "train loss:0.4197478307648822\n",
      "train loss:0.5042876843846442\n",
      "train loss:0.38508166278532047\n",
      "train loss:0.40558134376247224\n",
      "train loss:0.4540091998273864\n",
      "train loss:0.4781149969598338\n",
      "train loss:0.34255314032019457\n",
      "train loss:0.3933741481827105\n",
      "train loss:0.5485647636603526\n",
      "train loss:0.2976381367193536\n",
      "train loss:0.41383612879341614\n",
      "train loss:0.46366131169438873\n",
      "train loss:0.46417294097581363\n",
      "train loss:0.3395654356606707\n",
      "train loss:0.48323639841886723\n",
      "train loss:0.35181709241004167\n",
      "train loss:0.4524964908403993\n",
      "train loss:0.3756262898248595\n",
      "train loss:0.41273442969025065\n",
      "train loss:0.25890134297199724\n",
      "train loss:0.3958630487990096\n",
      "train loss:0.3535003074698176\n",
      "train loss:0.4804359858166248\n",
      "train loss:0.2945126810357375\n",
      "train loss:0.28775724245082474\n",
      "train loss:0.3279094412212403\n",
      "train loss:0.36069879592539444\n",
      "train loss:0.2897800339113644\n",
      "train loss:0.3020466227149037\n",
      "train loss:0.5238020587225701\n",
      "train loss:0.4641130808289563\n",
      "train loss:0.5565147370139685\n",
      "train loss:0.302803758282297\n",
      "train loss:0.46102113413024415\n",
      "train loss:0.23439543116458933\n",
      "train loss:0.3608391962718986\n",
      "train loss:0.4418879117244049\n",
      "train loss:0.31583082754990227\n",
      "train loss:0.49643737926458686\n",
      "train loss:0.36692082921571606\n",
      "train loss:0.40936871493541976\n",
      "train loss:0.43023004239708484\n",
      "train loss:0.3363987152818471\n",
      "train loss:0.35727688103663896\n",
      "train loss:0.359067063284675\n",
      "train loss:0.4752120893182731\n",
      "train loss:0.42354942191666284\n",
      "train loss:0.3557254868751008\n",
      "train loss:0.41338721569250025\n",
      "train loss:0.430135181970085\n",
      "train loss:0.3451388596014096\n",
      "train loss:0.3638642198024882\n",
      "train loss:0.2848721895369752\n",
      "train loss:0.32403569782348773\n",
      "train loss:0.3764195865943916\n",
      "train loss:0.43180596492837997\n",
      "train loss:0.2727975570540338\n",
      "train loss:0.42376379810009523\n",
      "train loss:0.4229582273219257\n",
      "train loss:0.40410282728884056\n",
      "train loss:0.371028329109101\n",
      "train loss:0.3949416723257457\n",
      "train loss:0.3498124682760376\n",
      "train loss:0.47793473634727623\n",
      "train loss:0.3536735950364332\n",
      "train loss:0.5331678928173266\n",
      "train loss:0.46372126513879564\n",
      "train loss:0.4157267128679966\n",
      "train loss:0.37293571781966706\n",
      "train loss:0.3114444660855835\n",
      "train loss:0.40819601846547565\n",
      "train loss:0.3821798896947164\n",
      "train loss:0.2776115197078672\n",
      "train loss:0.3125537981204347\n",
      "train loss:0.3172696277047976\n",
      "train loss:0.397321192780504\n",
      "train loss:0.25982340318805053\n",
      "train loss:0.42851072265300816\n",
      "train loss:0.3483892122737195\n",
      "train loss:0.45561448565937995\n",
      "train loss:0.5083361646486824\n",
      "train loss:0.3289239820198563\n",
      "train loss:0.30859388163091106\n",
      "train loss:0.40299505548076664\n",
      "train loss:0.42260509393897655\n",
      "train loss:0.28298336796224627\n",
      "train loss:0.4166924021857535\n",
      "train loss:0.3821428033214449\n",
      "train loss:0.2874569182558714\n",
      "train loss:0.2982478571780663\n",
      "train loss:0.2477341329270636\n",
      "train loss:0.3167311516650443\n",
      "train loss:0.3182416083209072\n",
      "train loss:0.3277783400136915\n",
      "train loss:0.2729836763238059\n",
      "train loss:0.416651482784583\n",
      "train loss:0.5344180543761962\n",
      "train loss:0.40441744416419007\n",
      "train loss:0.33572764076281\n",
      "train loss:0.31250932803931425\n",
      "train loss:0.3245100063731249\n",
      "train loss:0.31715222900150936\n",
      "train loss:0.3813126621934474\n",
      "train loss:0.3328992803505575\n",
      "train loss:0.3365660983172489\n",
      "train loss:0.3561882816882291\n",
      "train loss:0.3575498563575539\n",
      "train loss:0.5176366952307476\n",
      "train loss:0.47341673399746953\n",
      "train loss:0.30505927498566293\n",
      "train loss:0.4269412073636146\n",
      "train loss:0.2942781229908081\n",
      "train loss:0.4905037398494179\n",
      "train loss:0.3007062835835293\n",
      "train loss:0.2555139082951129\n",
      "train loss:0.26022478041977776\n",
      "train loss:0.36909848452137317\n",
      "train loss:0.5544342719577388\n",
      "train loss:0.25485870224878904\n",
      "train loss:0.29204337071372816\n",
      "train loss:0.34879602102457613\n",
      "train loss:0.31974209754275856\n",
      "train loss:0.38545367374376965\n",
      "train loss:0.4286643972298981\n",
      "train loss:0.4116085409318076\n",
      "train loss:0.44290088005703704\n",
      "train loss:0.39869362358845595\n",
      "train loss:0.23579204313001761\n",
      "train loss:0.3880181931471651\n",
      "train loss:0.24293944424554292\n",
      "train loss:0.4240463567810498\n",
      "train loss:0.31932763042296636\n",
      "train loss:0.4536044172050876\n",
      "train loss:0.43854219678516615\n",
      "train loss:0.5336261825100359\n",
      "train loss:0.2793778577711244\n",
      "train loss:0.29284285661490184\n",
      "train loss:0.3332692003780652\n",
      "train loss:0.3412396871019043\n",
      "train loss:0.5085468141545111\n",
      "train loss:0.3276279619591081\n",
      "train loss:0.42863517654161565\n",
      "train loss:0.41736636113019926\n",
      "train loss:0.3398780827447827\n",
      "train loss:0.3406694315011148\n",
      "train loss:0.3093943375030989\n",
      "train loss:0.3231117597183808\n",
      "train loss:0.2595759627311391\n",
      "train loss:0.49364473176660256\n",
      "train loss:0.39458657067537745\n",
      "train loss:0.4209740137168419\n",
      "train loss:0.3713836292534276\n",
      "train loss:0.4105897719818582\n",
      "train loss:0.3474341120233707\n",
      "train loss:0.3908475670183285\n",
      "train loss:0.43023357615602714\n",
      "train loss:0.3853885056762316\n",
      "train loss:0.22087626706822333\n",
      "train loss:0.2995436104530175\n",
      "train loss:0.30594877979416396\n",
      "train loss:0.3777168582227961\n",
      "train loss:0.4835952716685518\n",
      "train loss:0.35834120370254624\n",
      "train loss:0.4129922564631103\n",
      "train loss:0.27105464442882105\n",
      "train loss:0.24085174733741785\n",
      "train loss:0.27180597427277414\n",
      "train loss:0.2813960275197905\n",
      "train loss:0.40034277395411855\n",
      "train loss:0.36557438681436916\n",
      "train loss:0.3280345703022226\n",
      "train loss:0.36711148942003946\n",
      "train loss:0.5253102670140737\n",
      "train loss:0.24576959446364807\n",
      "train loss:0.40422450556139666\n",
      "train loss:0.4147313477147574\n",
      "train loss:0.34447430074355345\n",
      "train loss:0.33061767241212053\n",
      "train loss:0.3553189147750425\n",
      "train loss:0.6542533608500579\n",
      "train loss:0.4956671220651955\n",
      "train loss:0.315426408679511\n",
      "train loss:0.33813182914118295\n",
      "train loss:0.2913460656828508\n",
      "train loss:0.371407750280342\n",
      "train loss:0.35136960371864434\n",
      "train loss:0.37459761696086574\n",
      "train loss:0.5428637254265144\n",
      "train loss:0.5176111250188588\n",
      "train loss:0.49084452684293917\n",
      "train loss:0.3738810105038464\n",
      "train loss:0.283574263417881\n",
      "train loss:0.4699173145740322\n",
      "train loss:0.32433444787173216\n",
      "train loss:0.33619303673211953\n",
      "train loss:0.3135234852914461\n",
      "train loss:0.1989599386432007\n",
      "train loss:0.3309011150179489\n",
      "train loss:0.3453920813889772\n",
      "train loss:0.24516403129716097\n",
      "train loss:0.26876860544940745\n",
      "train loss:0.24418498031036595\n",
      "train loss:0.30937068181769567\n",
      "train loss:0.4302455634265936\n",
      "train loss:0.20864569686197373\n",
      "train loss:0.35291801177841314\n",
      "train loss:0.30327470990285976\n",
      "train loss:0.3914880730228353\n",
      "train loss:0.5376813780844294\n",
      "train loss:0.29261061973121105\n",
      "train loss:0.34610624561308817\n",
      "train loss:0.27436219599299166\n",
      "train loss:0.3185847126743759\n",
      "train loss:0.24280098896629326\n",
      "train loss:0.3185165567661778\n",
      "train loss:0.36734815479390565\n",
      "train loss:0.15478277329817158\n",
      "train loss:0.3687179446211909\n",
      "train loss:0.45936066418479526\n",
      "train loss:0.4308490747152268\n",
      "train loss:0.43634731570310736\n",
      "train loss:0.4279331780015479\n",
      "train loss:0.35930576873987563\n",
      "train loss:0.3045446494139018\n",
      "train loss:0.41868093824432295\n",
      "train loss:0.29122113583803566\n",
      "train loss:0.5490109765198032\n",
      "train loss:0.4022545957001957\n",
      "train loss:0.46642537468950224\n",
      "train loss:0.4646634364885297\n",
      "train loss:0.3099293187481018\n",
      "train loss:0.30017218314214933\n",
      "train loss:0.38345882011473087\n",
      "train loss:0.32118335522797\n",
      "train loss:0.43624369125384244\n",
      "train loss:0.40003552782931584\n",
      "train loss:0.46353398052813555\n",
      "train loss:0.3784029189790871\n",
      "train loss:0.4788168816761308\n",
      "train loss:0.46775250881674874\n",
      "train loss:0.27587569892952685\n",
      "train loss:0.4203774197815109\n",
      "train loss:0.30075526330187463\n",
      "train loss:0.2743843467883664\n",
      "train loss:0.36502101905228185\n",
      "train loss:0.4638269710847897\n",
      "train loss:0.43393362743701774\n",
      "train loss:0.3649698935037568\n",
      "train loss:0.32756708377701566\n",
      "train loss:0.4007292732499719\n",
      "train loss:0.26624792500292643\n",
      "train loss:0.4526438853017616\n",
      "train loss:0.2550749709986416\n",
      "train loss:0.293951077972645\n",
      "train loss:0.41095904508508274\n",
      "train loss:0.41312507042475616\n",
      "train loss:0.34145286828001736\n",
      "train loss:0.2790546658298285\n",
      "train loss:0.35683134411301376\n",
      "train loss:0.42336110626042667\n",
      "train loss:0.3179700230582079\n",
      "train loss:0.4426068163740112\n",
      "train loss:0.312457605165458\n",
      "train loss:0.2796353873378784\n",
      "train loss:0.28948797757117517\n",
      "train loss:0.3779242358007769\n",
      "train loss:0.3781480574759094\n",
      "train loss:0.3558327112377414\n",
      "train loss:0.22974817297567218\n",
      "train loss:0.25362209744558656\n",
      "train loss:0.25665775654988826\n",
      "train loss:0.21491978354709493\n",
      "train loss:0.4297591383133762\n",
      "train loss:0.272761887410192\n",
      "train loss:0.40673087543060477\n",
      "train loss:0.3482699054018073\n",
      "train loss:0.552665375669063\n",
      "train loss:0.2874582792849203\n",
      "train loss:0.46541281943523893\n",
      "train loss:0.32532299848207297\n",
      "train loss:0.3247360696775584\n",
      "train loss:0.37022577763010006\n",
      "train loss:0.3514922108252883\n",
      "train loss:0.3242723952865507\n",
      "train loss:0.4165302271506706\n",
      "train loss:0.35020738290824105\n",
      "train loss:0.278659854604508\n",
      "train loss:0.37021692235239995\n",
      "train loss:0.21251092361973445\n",
      "train loss:0.2780997893613618\n",
      "train loss:0.4544813964303177\n",
      "train loss:0.491908449855456\n",
      "train loss:0.40317201641858674\n",
      "train loss:0.4610606188610302\n",
      "train loss:0.36853602408671016\n",
      "train loss:0.4028616327356965\n",
      "train loss:0.5234272588498682\n",
      "train loss:0.3582796907900272\n",
      "train loss:0.40373158980462626\n",
      "train loss:0.3414485372273829\n",
      "train loss:0.2946127490166037\n",
      "train loss:0.5078901398760504\n",
      "train loss:0.34866589945692483\n",
      "train loss:0.6209043601039873\n",
      "train loss:0.3406196046503321\n",
      "train loss:0.3671841447649223\n",
      "train loss:0.34661502539908495\n",
      "train loss:0.23463125718873126\n",
      "train loss:0.227369475046879\n",
      "train loss:0.346283230436165\n",
      "train loss:0.3256019997778485\n",
      "train loss:0.3312559798521959\n",
      "train loss:0.29526827846680037\n",
      "train loss:0.32461015428758627\n",
      "train loss:0.2791670720001928\n",
      "train loss:0.24482791302428264\n",
      "train loss:0.28285336175327663\n",
      "train loss:0.26003572400034186\n",
      "train loss:0.5117957722929158\n",
      "train loss:0.3290628460735381\n",
      "train loss:0.387923049243069\n",
      "train loss:0.32714038798881157\n",
      "train loss:0.2738065894170882\n",
      "train loss:0.3441093460689071\n",
      "train loss:0.31433235078530414\n",
      "train loss:0.4288192982338394\n",
      "=== epoch:3, train acc:0.89, test acc:0.869 ===\n",
      "train loss:0.40574381742245774\n",
      "train loss:0.3547253845566617\n",
      "train loss:0.3066147783421059\n",
      "train loss:0.19713154734814853\n",
      "train loss:0.47040226963539716\n",
      "train loss:0.33490260312233494\n",
      "train loss:0.30950618773389404\n",
      "train loss:0.29369616830088563\n",
      "train loss:0.3300256183864078\n",
      "train loss:0.23737927745197848\n",
      "train loss:0.232414044370086\n",
      "train loss:0.48255777628282653\n",
      "train loss:0.320436710948604\n",
      "train loss:0.2484924125215454\n",
      "train loss:0.2816390405026786\n",
      "train loss:0.39082066519216413\n",
      "train loss:0.4824466022757166\n",
      "train loss:0.3576871296164124\n",
      "train loss:0.4822631118025695\n",
      "train loss:0.419119679538631\n",
      "train loss:0.2399847129292137\n",
      "train loss:0.25067302292213706\n",
      "train loss:0.22645719078966928\n",
      "train loss:0.26123901629067925\n",
      "train loss:0.3033012536551908\n",
      "train loss:0.28420067451178455\n",
      "train loss:0.27171793297796887\n",
      "train loss:0.33754443707434584\n",
      "train loss:0.43692639893794827\n",
      "train loss:0.35947107382368415\n",
      "train loss:0.2748509488023092\n",
      "train loss:0.24855063362999594\n",
      "train loss:0.2747834449762122\n",
      "train loss:0.23009528396896198\n",
      "train loss:0.37268072355208354\n",
      "train loss:0.1985473792060838\n",
      "train loss:0.37336351860594164\n",
      "train loss:0.3344724155079929\n",
      "train loss:0.42831651353506545\n",
      "train loss:0.34139295050555096\n",
      "train loss:0.3803374024602698\n",
      "train loss:0.3176598103363613\n",
      "train loss:0.36247356945222436\n",
      "train loss:0.23381264147749936\n",
      "train loss:0.31000084884734147\n",
      "train loss:0.3444685165253299\n",
      "train loss:0.25757675486781767\n",
      "train loss:0.311712563546194\n",
      "train loss:0.3035497885478259\n",
      "train loss:0.30543730216921394\n",
      "train loss:0.36105303057720595\n",
      "train loss:0.29879649274684383\n",
      "train loss:0.4317589132725338\n",
      "train loss:0.30540689491820516\n",
      "train loss:0.34504597800446773\n",
      "train loss:0.38253382562403504\n",
      "train loss:0.2990938519836718\n",
      "train loss:0.29429583080187866\n",
      "train loss:0.20790610673723972\n",
      "train loss:0.3513655702559367\n",
      "train loss:0.29279324794823003\n",
      "train loss:0.5165527242228252\n",
      "train loss:0.4732579509706568\n",
      "train loss:0.2763424653129763\n",
      "train loss:0.21278127501920488\n",
      "train loss:0.3001694356988135\n",
      "train loss:0.32027152044666435\n",
      "train loss:0.2564670922343105\n",
      "train loss:0.29934441787272237\n",
      "train loss:0.33760203288621843\n",
      "train loss:0.4563626821007318\n",
      "train loss:0.3569999193483133\n",
      "train loss:0.3238852218068367\n",
      "train loss:0.2932655341908383\n",
      "train loss:0.22713495095227706\n",
      "train loss:0.3354160168652205\n",
      "train loss:0.267062124376933\n",
      "train loss:0.2209260289772174\n",
      "train loss:0.35269167021813913\n",
      "train loss:0.4067326540185816\n",
      "train loss:0.4319920082221343\n",
      "train loss:0.2949715180340297\n",
      "train loss:0.3918459373105977\n",
      "train loss:0.38958277367352445\n",
      "train loss:0.29986123135162324\n",
      "train loss:0.3120333628279909\n",
      "train loss:0.3042660602614411\n",
      "train loss:0.39267627685636314\n",
      "train loss:0.48080139477224937\n",
      "train loss:0.6271508797865385\n",
      "train loss:0.3250498915912216\n",
      "train loss:0.4285706370953705\n",
      "train loss:0.2684657695180385\n",
      "train loss:0.33197513444643106\n",
      "train loss:0.3684614229620381\n",
      "train loss:0.28079160433832223\n",
      "train loss:0.49666245717319973\n",
      "train loss:0.38147801828355454\n",
      "train loss:0.3659736398427595\n",
      "train loss:0.35108516243264426\n",
      "train loss:0.25996343659404647\n",
      "train loss:0.37008153029368157\n",
      "train loss:0.38009062961557644\n",
      "train loss:0.31866524523173906\n",
      "train loss:0.3580534760850973\n",
      "train loss:0.26209832193050814\n",
      "train loss:0.2615363963964943\n",
      "train loss:0.21822861322904455\n",
      "train loss:0.22988235342114327\n",
      "train loss:0.4385260193390984\n",
      "train loss:0.3362260685778084\n",
      "train loss:0.3062701813018429\n",
      "train loss:0.27793130860326404\n",
      "train loss:0.29794720149474674\n",
      "train loss:0.46134304294746475\n",
      "train loss:0.2835595366225483\n",
      "train loss:0.3999280789769069\n",
      "train loss:0.28637439841820056\n",
      "train loss:0.30652685648821887\n",
      "train loss:0.27817714148586037\n",
      "train loss:0.33773543060018957\n",
      "train loss:0.3864020361337131\n",
      "train loss:0.3081227673377453\n",
      "train loss:0.3418863403217251\n",
      "train loss:0.3306292879476914\n",
      "train loss:0.26529456662544804\n",
      "train loss:0.5304961884148639\n",
      "train loss:0.33616918620810965\n",
      "train loss:0.2330744805829503\n",
      "train loss:0.2306516654345432\n",
      "train loss:0.2756671627927578\n",
      "train loss:0.2585533047742561\n",
      "train loss:0.22960038570964716\n",
      "train loss:0.3808135575467475\n",
      "train loss:0.30808860414345285\n",
      "train loss:0.3133973629643328\n",
      "train loss:0.32383560858562527\n",
      "train loss:0.43106563927594627\n",
      "train loss:0.349813986700404\n",
      "train loss:0.4167322867009268\n",
      "train loss:0.28292736256833706\n",
      "train loss:0.2157091465459911\n",
      "train loss:0.3269937681260522\n",
      "train loss:0.42712190780849824\n",
      "train loss:0.41099402910384536\n",
      "train loss:0.3002239520151986\n",
      "train loss:0.1650644657783497\n",
      "train loss:0.39758926752828555\n",
      "train loss:0.32906278739233774\n",
      "train loss:0.25605820978058164\n",
      "train loss:0.33204783901624785\n",
      "train loss:0.4034171812609749\n",
      "train loss:0.350104932738615\n",
      "train loss:0.23499153521730906\n",
      "train loss:0.5062404914958872\n",
      "train loss:0.24898985935027704\n",
      "train loss:0.3565869658729685\n",
      "train loss:0.28132302500257617\n",
      "train loss:0.5291290376241509\n",
      "train loss:0.43056375711274314\n",
      "train loss:0.2630522806574776\n",
      "train loss:0.28624362705648404\n",
      "train loss:0.38757803798861606\n",
      "train loss:0.38910549657554144\n",
      "train loss:0.3569202158005665\n",
      "train loss:0.35699751244138794\n",
      "train loss:0.37028063768505404\n",
      "train loss:0.338851772807315\n",
      "train loss:0.2698250831842042\n",
      "train loss:0.3248434082820883\n",
      "train loss:0.2848295028699958\n",
      "train loss:0.3261621170263703\n",
      "train loss:0.26627815118258075\n",
      "train loss:0.30752985249101955\n",
      "train loss:0.25856737651175277\n",
      "train loss:0.4117273886599074\n",
      "train loss:0.3750454276828563\n",
      "train loss:0.4392067911460184\n",
      "train loss:0.2438417880768149\n",
      "train loss:0.2891364261090396\n",
      "train loss:0.3329126829402471\n",
      "train loss:0.4667559048912255\n",
      "train loss:0.37509884743811706\n",
      "train loss:0.43390245541010286\n",
      "train loss:0.24302710851204476\n",
      "train loss:0.3071161739751199\n",
      "train loss:0.24399446451773843\n",
      "train loss:0.2117975215165318\n",
      "train loss:0.34453345471696695\n",
      "train loss:0.4005626542392142\n",
      "train loss:0.2623934984271578\n",
      "train loss:0.22978926528203023\n",
      "train loss:0.2993254486130347\n",
      "train loss:0.3952892574054209\n",
      "train loss:0.21275466178598962\n",
      "train loss:0.29970197784538827\n",
      "train loss:0.404105479176166\n",
      "train loss:0.4024694076863021\n",
      "train loss:0.2520589124662081\n",
      "train loss:0.33354662297288196\n",
      "train loss:0.31980172679099916\n",
      "train loss:0.3178835890138459\n",
      "train loss:0.4395012093133082\n",
      "train loss:0.2753684928855837\n",
      "train loss:0.2620997390261561\n",
      "train loss:0.3414976006509584\n",
      "train loss:0.2969812926326879\n",
      "train loss:0.37102260845140017\n",
      "train loss:0.4799195929487298\n",
      "train loss:0.4249495293447081\n",
      "train loss:0.3718270203618217\n",
      "train loss:0.43507750671047957\n",
      "train loss:0.29603182879007267\n",
      "train loss:0.35305798385518794\n",
      "train loss:0.34345383448770844\n",
      "train loss:0.28253924271292585\n",
      "train loss:0.3074563994780218\n",
      "train loss:0.34219087887229455\n",
      "train loss:0.26576707930402116\n",
      "train loss:0.26443051562210257\n",
      "train loss:0.335268006779272\n",
      "train loss:0.36332378046714814\n",
      "train loss:0.2716057213092653\n",
      "train loss:0.403518883465595\n",
      "train loss:0.33400962019968494\n",
      "train loss:0.21753521605848591\n",
      "train loss:0.34532886641545835\n",
      "train loss:0.4386665563492091\n",
      "train loss:0.2798538762690691\n",
      "train loss:0.2697706530798283\n",
      "train loss:0.24838978493870878\n",
      "train loss:0.27844502004455907\n",
      "train loss:0.3238526256240712\n",
      "train loss:0.42772023367417034\n",
      "train loss:0.23308513117582808\n",
      "train loss:0.3624962778182939\n",
      "train loss:0.48147031647264377\n",
      "train loss:0.22312267158942992\n",
      "train loss:0.2442175686863196\n",
      "train loss:0.35243103171533263\n",
      "train loss:0.45405288319376297\n",
      "train loss:0.5163810118709351\n",
      "train loss:0.3611158800453046\n",
      "train loss:0.49214605733239547\n",
      "train loss:0.21379347590242895\n",
      "train loss:0.1955918404978924\n",
      "train loss:0.30288783059375396\n",
      "train loss:0.40500964556539\n",
      "train loss:0.31048193096392507\n",
      "train loss:0.295696300950264\n",
      "train loss:0.3705410753555814\n",
      "train loss:0.17391665525017536\n",
      "train loss:0.366660157613161\n",
      "train loss:0.42261485209224475\n",
      "train loss:0.23280084092878162\n",
      "train loss:0.5072823563504275\n",
      "train loss:0.4089025309505442\n",
      "train loss:0.20855072663595792\n",
      "train loss:0.23302268193598366\n",
      "train loss:0.38541738981050616\n",
      "train loss:0.19489745841058642\n",
      "train loss:0.35699402674707054\n",
      "train loss:0.3572181494635989\n",
      "train loss:0.26791798285594665\n",
      "train loss:0.3560326298388599\n",
      "train loss:0.45387356231928083\n",
      "train loss:0.3335499646178711\n",
      "train loss:0.3274593175041785\n",
      "train loss:0.28793044853113303\n",
      "train loss:0.3404884470484407\n",
      "train loss:0.2738402493087078\n",
      "train loss:0.33392457646106666\n",
      "train loss:0.3599457834300944\n",
      "train loss:0.28643338177014016\n",
      "train loss:0.2528821222646427\n",
      "train loss:0.3190070209246392\n",
      "train loss:0.3484239196053868\n",
      "train loss:0.25642537060943654\n",
      "train loss:0.446335117539608\n",
      "train loss:0.241971989725272\n",
      "train loss:0.38853342158130727\n",
      "train loss:0.23538093176868455\n",
      "train loss:0.47815884786695206\n",
      "train loss:0.32036146328223536\n",
      "train loss:0.35560387013349\n",
      "train loss:0.30056172955186744\n",
      "train loss:0.32978924955835687\n",
      "train loss:0.256424072753476\n",
      "train loss:0.32587073976202113\n",
      "train loss:0.3665717378098865\n",
      "train loss:0.31560652870578687\n",
      "train loss:0.3149755152533871\n",
      "train loss:0.19601417959314518\n",
      "train loss:0.2537425974985641\n",
      "train loss:0.2225889038286452\n",
      "train loss:0.32731979426981284\n",
      "train loss:0.25075101713462356\n",
      "train loss:0.49391935472065507\n",
      "train loss:0.40685485368718466\n",
      "train loss:0.3654642180121685\n",
      "train loss:0.3684000920128674\n",
      "train loss:0.2881440939831305\n",
      "train loss:0.3040682818734713\n",
      "train loss:0.4330369175825142\n",
      "train loss:0.4285593440825331\n",
      "train loss:0.2604791002403475\n",
      "train loss:0.32971474137753387\n",
      "train loss:0.48019650724728113\n",
      "train loss:0.2831141340383704\n",
      "train loss:0.4101850369944284\n",
      "train loss:0.3411358643771598\n",
      "train loss:0.33600614123094547\n",
      "train loss:0.19920010674404406\n",
      "train loss:0.41930327523365596\n",
      "train loss:0.3399816822397884\n",
      "train loss:0.22988420336748355\n",
      "train loss:0.29417623476211147\n",
      "train loss:0.300558008544489\n",
      "train loss:0.3448077061049694\n",
      "train loss:0.23163769342512844\n",
      "train loss:0.38732528946430006\n",
      "train loss:0.3810598669084335\n",
      "train loss:0.25364016878595735\n",
      "train loss:0.3986713068486112\n",
      "train loss:0.3762503454462292\n",
      "train loss:0.43870101009540563\n",
      "train loss:0.5020886277167834\n",
      "train loss:0.23671222625914923\n",
      "train loss:0.26190207926552533\n",
      "train loss:0.3561269732994184\n",
      "train loss:0.21635873944839867\n",
      "train loss:0.2239920342531786\n",
      "train loss:0.2552014634348943\n",
      "train loss:0.30474097374320613\n",
      "train loss:0.5092027708821023\n",
      "train loss:0.3736112991244833\n",
      "train loss:0.3481701117821516\n",
      "train loss:0.40231542195521636\n",
      "train loss:0.23410105040169837\n",
      "train loss:0.21080076065194714\n",
      "train loss:0.25240141530347293\n",
      "train loss:0.25843079728323115\n",
      "train loss:0.2965439531993255\n",
      "train loss:0.33297394824874077\n",
      "train loss:0.4086035224987458\n",
      "train loss:0.329797858799307\n",
      "train loss:0.43920674348903405\n",
      "train loss:0.2260665488629824\n",
      "train loss:0.2214026420043499\n",
      "train loss:0.29342961902688924\n",
      "train loss:0.24517847584286442\n",
      "train loss:0.24525550845638816\n",
      "train loss:0.247422995748362\n",
      "train loss:0.2981083803508472\n",
      "train loss:0.30667624300133484\n",
      "train loss:0.25486740159064764\n",
      "train loss:0.3763196225084776\n",
      "train loss:0.3182300000486322\n",
      "train loss:0.36892660728120463\n",
      "train loss:0.39048687581562624\n",
      "train loss:0.2686478300504996\n",
      "train loss:0.22684552555971899\n",
      "train loss:0.3400189587092104\n",
      "train loss:0.29412364394510926\n",
      "train loss:0.40316865234408333\n",
      "train loss:0.21654733442856652\n",
      "train loss:0.33596199978192404\n",
      "train loss:0.38562189343064274\n",
      "train loss:0.2659681384770795\n",
      "train loss:0.2714875567511052\n",
      "train loss:0.31756804673331057\n",
      "train loss:0.28674506138875144\n",
      "train loss:0.2904535929621915\n",
      "train loss:0.30982847937343894\n",
      "train loss:0.28768179338897737\n",
      "train loss:0.29236375483852456\n",
      "train loss:0.35103846338742706\n",
      "train loss:0.470622247151015\n",
      "train loss:0.39387983928444315\n",
      "train loss:0.29333167805902205\n",
      "train loss:0.48146215206178283\n",
      "train loss:0.4123458285141297\n",
      "train loss:0.2990480175242695\n",
      "train loss:0.33972736078350374\n",
      "train loss:0.3640173819790982\n",
      "train loss:0.3467574214645377\n",
      "train loss:0.29694138093415323\n",
      "train loss:0.20213815678812241\n",
      "train loss:0.2782041153434089\n",
      "train loss:0.31928852252525686\n",
      "train loss:0.24829377190511903\n",
      "train loss:0.39424662223515883\n",
      "train loss:0.3579983983079774\n",
      "train loss:0.3512749399810964\n",
      "train loss:0.323936537397452\n",
      "train loss:0.21916601532854088\n",
      "train loss:0.33176378162163167\n",
      "train loss:0.5784197308429898\n",
      "train loss:0.3342925069730774\n",
      "train loss:0.3180896920454927\n",
      "train loss:0.3351118821158743\n",
      "train loss:0.35867561781251245\n",
      "train loss:0.30870640418920264\n",
      "train loss:0.32259026689190184\n",
      "train loss:0.28054275443772014\n",
      "train loss:0.3403134710837468\n",
      "train loss:0.3032124156715461\n",
      "train loss:0.4548779629861628\n",
      "train loss:0.2286659504010284\n",
      "train loss:0.43389207320751777\n",
      "train loss:0.30779447021154305\n",
      "train loss:0.31986074262638003\n",
      "train loss:0.3962294974082682\n",
      "train loss:0.2752222564970491\n",
      "train loss:0.47229126291765167\n",
      "train loss:0.40028261894552164\n",
      "train loss:0.2026325959603878\n",
      "train loss:0.30625509438374343\n",
      "train loss:0.25635649052191584\n",
      "train loss:0.25390390800737567\n",
      "train loss:0.5680329888353512\n",
      "train loss:0.30903688999781886\n",
      "train loss:0.25967440672014597\n",
      "train loss:0.4624681463624716\n",
      "train loss:0.320470849269612\n",
      "train loss:0.4690410550879453\n",
      "train loss:0.4270200467228522\n",
      "train loss:0.3172504011471458\n",
      "train loss:0.28913039047050865\n",
      "train loss:0.3349270697901223\n",
      "train loss:0.2736509559219194\n",
      "train loss:0.2865971713058943\n",
      "train loss:0.280684314262043\n",
      "train loss:0.3422189592288034\n",
      "train loss:0.27287425428655404\n",
      "train loss:0.4072657870644713\n",
      "train loss:0.35207282322027406\n",
      "train loss:0.35368629351484965\n",
      "train loss:0.33783816124512056\n",
      "train loss:0.2140036244178785\n",
      "train loss:0.20100159557706576\n",
      "train loss:0.3372411182034029\n",
      "train loss:0.3046392711737186\n",
      "train loss:0.40330607236628346\n",
      "train loss:0.36531942785717936\n",
      "train loss:0.34481881046360696\n",
      "train loss:0.20886178289042812\n",
      "train loss:0.2833164785717577\n",
      "train loss:0.251404038077127\n",
      "train loss:0.1767933814801457\n",
      "train loss:0.45435690596828027\n",
      "train loss:0.2909369941188506\n",
      "train loss:0.3642416880398022\n",
      "train loss:0.32224668619596153\n",
      "train loss:0.3053978195499397\n",
      "train loss:0.26552650364825575\n",
      "train loss:0.5439623374122816\n",
      "train loss:0.2538013538733603\n",
      "train loss:0.3230859050303236\n",
      "train loss:0.24051201266087374\n",
      "train loss:0.3543736143149562\n",
      "train loss:0.21471857032167738\n",
      "train loss:0.40621396461920084\n",
      "train loss:0.37903338293269107\n",
      "train loss:0.3895923590121302\n",
      "train loss:0.29928444607574284\n",
      "train loss:0.17796898021149452\n",
      "train loss:0.4541682282176327\n",
      "train loss:0.2388994477154267\n",
      "train loss:0.3174092037726257\n",
      "train loss:0.31158026904856917\n",
      "train loss:0.28347563477070403\n",
      "train loss:0.30586041775010214\n",
      "train loss:0.24629889464017332\n",
      "train loss:0.24150752099423936\n",
      "train loss:0.22110351655040275\n",
      "train loss:0.27394974857661014\n",
      "train loss:0.3480690783523607\n",
      "train loss:0.21580487260713405\n",
      "train loss:0.2599016910688416\n",
      "train loss:0.2776325625466544\n",
      "train loss:0.1984720269760883\n",
      "train loss:0.25199083474861994\n",
      "train loss:0.1946729628152184\n",
      "train loss:0.3792263856216072\n",
      "train loss:0.2496017236388576\n",
      "train loss:0.5173112299262308\n",
      "train loss:0.5112300346791185\n",
      "train loss:0.2748556647169306\n",
      "train loss:0.4309380442736062\n",
      "train loss:0.2634508071742305\n",
      "train loss:0.2931882999782707\n",
      "train loss:0.32547367996429794\n",
      "train loss:0.3045734444235519\n",
      "train loss:0.37756400285933833\n",
      "train loss:0.25705389372123283\n",
      "train loss:0.2418780995593005\n",
      "train loss:0.25375836410386327\n",
      "train loss:0.19934115303245517\n",
      "train loss:0.20196595184511434\n",
      "train loss:0.35425825193904087\n",
      "train loss:0.21972396909759467\n",
      "train loss:0.2569826189788267\n",
      "train loss:0.32567258174775376\n",
      "train loss:0.20255791696851586\n",
      "train loss:0.19564258161487147\n",
      "train loss:0.24082117284939042\n",
      "train loss:0.31175769547150356\n",
      "train loss:0.25567108099556946\n",
      "train loss:0.3724179451060992\n",
      "train loss:0.3941578594421874\n",
      "train loss:0.3719317523183808\n",
      "train loss:0.24425008658726402\n",
      "train loss:0.30838341655428503\n",
      "train loss:0.30609491544743495\n",
      "train loss:0.3860251551067615\n",
      "train loss:0.22053824765162006\n",
      "train loss:0.21785509514819526\n",
      "train loss:0.3061682066726964\n",
      "train loss:0.21417423559178345\n",
      "train loss:0.25526238326920825\n",
      "train loss:0.33333219388901114\n",
      "train loss:0.20328122097573703\n",
      "train loss:0.18814701208029722\n",
      "train loss:0.42740797785455065\n",
      "train loss:0.2658638699671139\n",
      "train loss:0.34124811676656724\n",
      "train loss:0.2194870831024888\n",
      "train loss:0.2585289089934967\n",
      "train loss:0.4070661838022624\n",
      "train loss:0.41080448356232735\n",
      "train loss:0.4645769053703113\n",
      "train loss:0.36276453991256585\n",
      "train loss:0.35242355418403337\n",
      "train loss:0.2392177674041541\n",
      "train loss:0.36056930751465444\n",
      "train loss:0.35133637300639975\n",
      "train loss:0.30808362675318596\n",
      "train loss:0.2818736143845106\n",
      "train loss:0.34778161842872274\n",
      "train loss:0.3543269576921542\n",
      "train loss:0.28236925532065715\n",
      "train loss:0.37635436728497546\n",
      "train loss:0.23715890876897308\n",
      "train loss:0.28840040224027946\n",
      "train loss:0.2965615426658769\n",
      "train loss:0.2634871165804398\n",
      "train loss:0.32230456269761704\n",
      "train loss:0.4198580377930471\n",
      "train loss:0.3161797388967915\n",
      "train loss:0.30855782504216145\n",
      "train loss:0.28542765968846245\n",
      "train loss:0.33870538610604894\n",
      "train loss:0.3463887419607296\n",
      "train loss:0.33216649920726765\n",
      "train loss:0.352161535361684\n",
      "train loss:0.322110261864985\n",
      "train loss:0.22335691918175435\n",
      "train loss:0.2658069482698644\n",
      "train loss:0.21342336600298037\n",
      "train loss:0.19993458847719833\n",
      "train loss:0.2880110311881284\n",
      "train loss:0.258398114217669\n",
      "train loss:0.24524305946564315\n",
      "train loss:0.3261812283015655\n",
      "train loss:0.23198580749648787\n",
      "train loss:0.2322266645457735\n",
      "train loss:0.08103696045365366\n",
      "train loss:0.4270437756674718\n",
      "train loss:0.2242396478957396\n",
      "train loss:0.2397217787841937\n",
      "train loss:0.4419099617047284\n",
      "train loss:0.32582053382762205\n",
      "train loss:0.3532976800017402\n",
      "train loss:0.34078543087223095\n",
      "train loss:0.5397149864851825\n",
      "train loss:0.2947009994327635\n",
      "train loss:0.29836387291636507\n",
      "train loss:0.4091118596142415\n",
      "train loss:0.22268805909024147\n",
      "train loss:0.26176466941708015\n",
      "train loss:0.3047798855539066\n",
      "train loss:0.3262265102294007\n",
      "train loss:0.2256670829129621\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/dl/common/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/dl/common/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train loss:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-aead5f1c58f6>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-aead5f1c58f6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/dl/common/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0marg_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2666\u001b[0m     \"\"\"\n\u001b[1;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2668\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f5859240bd93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SGD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acc_list' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(0, 10000, 600), train_acc_list)\n",
    "plt.title('SGD')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
